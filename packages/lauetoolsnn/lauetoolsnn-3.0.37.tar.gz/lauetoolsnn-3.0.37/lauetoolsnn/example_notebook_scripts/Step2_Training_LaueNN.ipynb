{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61bda473-6e51-4914-826e-21c50f0f61d1",
   "metadata": {},
   "source": [
    "# Notebook script for Training the neural network (supports single and two phase material)\n",
    "\n",
    "## Different steps of neural network training is outlined in this notebook (LaueToolsNN GUI does the same thing)\n",
    "\n",
    "### Load the data generated in Step 1\n",
    "### Define the Neural network architecture\n",
    "### Train the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2ad7bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import modules used for this Notebook\n",
    "import numpy as np\n",
    "import os\n",
    "import _pickle as cPickle\n",
    "import itertools\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## if LaueToolsNN is properly installed\n",
    "try:\n",
    "    from lauetoolsnn.utils_lauenn import array_generator, array_generator_verify, vali_array\n",
    "except:\n",
    "    # else import from a path where LaueToolsNN files are\n",
    "    import sys\n",
    "    sys.path.append(r\"C:\\Users\\purushot\\Desktop\\github_version_simple\\lauetoolsnn\")\n",
    "    from utils_lauenn import array_generator, array_generator_verify, vali_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa0a493-5a72-4c14-be8d-01d54f073260",
   "metadata": {},
   "source": [
    "## step 1: define material and path to access the training dataset generated using Step 1 script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ebdbbba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory where training dataset is stored is : C:\\Users\\purushot\\Desktop\\github_version_simple\\lauetoolsnn\\example_notebook_scripts//Cu\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "## User Input dictionary with parameters (reduced but same as the one used in STEP 1)\n",
    "## In case of only one phase/material, keep same value for material_ and material1_ key\n",
    "# =============================================================================\n",
    "input_params = {\n",
    "                \"material_\": \"Cu\",             ## same key as used in dict_LaueTools\n",
    "                \"material1_\": \"Cu\",            ## same key as used in dict_LaueTools\n",
    "                \"prefix\" : \"\",                 ## prefix for the folder to be created for training dataset\n",
    "                \"nb_grains_per_lp\" : 5,        ## max grains to be generated in a Laue Image\n",
    "                \"grains_nb_simulate\" : 100,    ## Number of orientations to generate (takes advantage of crystal symmetry)\n",
    "                \"batch_size\":50,               ## batches of files to use while training\n",
    "                \"epochs\":5,                    ## number of epochs for training\n",
    "                }\n",
    "\n",
    "material_= input_params[\"material_\"]\n",
    "material1_= input_params[\"material1_\"]\n",
    "nb_grains_per_lp = input_params[\"nb_grains_per_lp\"]\n",
    "grains_nb_simulate = input_params[\"grains_nb_simulate\"]\n",
    "\n",
    "if material_ != material1_:\n",
    "    save_directory = os.getcwd()+\"//\"+material_+\"_\"+material1_+input_params[\"prefix\"]\n",
    "else:\n",
    "    save_directory = os.getcwd()+\"//\"+material_+input_params[\"prefix\"]\n",
    "\n",
    "if not os.path.exists(save_directory):\n",
    "    print(\"The directory doesn't exists; please veify the path\")\n",
    "else:\n",
    "    print(\"Directory where training dataset is stored is : \"+save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be7b1f5-2ac1-45f6-8245-439ced9081d4",
   "metadata": {},
   "source": [
    "## Step 2: Load the necessary files generated in Step 1 script\n",
    "### Loading the Output class and ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4155ee36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900 31\n"
     ]
    }
   ],
   "source": [
    "classhkl = np.load(save_directory+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_0\"]\n",
    "angbins = np.load(save_directory+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_1\"]\n",
    "loc_new = np.load(save_directory+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_2\"]\n",
    "with open(save_directory+\"//class_weights.pickle\", \"rb\") as input_file:\n",
    "    class_weights = cPickle.load(input_file)\n",
    "class_weights = class_weights[0]\n",
    "\n",
    "n_bins = len(angbins)-1\n",
    "n_outputs = len(classhkl)\n",
    "print(n_bins, n_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8be2bdc-587e-47d0-b4c1-9d32fe78afc2",
   "metadata": {},
   "source": [
    "## Step 3: Defining a neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8e577ef-7f75-4ef0-b873-57607e34e6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import keras\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "metricsNN = [\n",
    "            keras.metrics.FalseNegatives(name=\"fn\"),\n",
    "            keras.metrics.FalsePositives(name=\"fp\"),\n",
    "            keras.metrics.TrueNegatives(name=\"tn\"),\n",
    "            keras.metrics.TruePositives(name=\"tp\"),\n",
    "            keras.metrics.Precision(name=\"precision\"),\n",
    "            keras.metrics.Recall(name=\"accuracy\"),\n",
    "            ]\n",
    "\n",
    "def model_arch_general(kernel_coeff = 0.0005, \n",
    "                       bias_coeff = 0.0005, \n",
    "                       init_mode='uniform',\n",
    "                       learning_rate=0.0001, \n",
    "                       neurons_multiplier= [900, 900, 900*7, 900*15, 31], \n",
    "                       layers=3, \n",
    "                       batch_norm=False, \n",
    "                       optimizer=\"adam\",\n",
    "                       activation='relu',\n",
    "                       dropout_rate=0.0, \n",
    "                       weight_constraint=None):\n",
    "    model = Sequential()\n",
    "    # Input layer\n",
    "    model.add(keras.Input(shape=(int(neurons_multiplier[0]),)))\n",
    "    \n",
    "    if layers > 0:\n",
    "        for lay in range(layers):\n",
    "            ## Hidden layer n\n",
    "            if kernel_coeff == None and bias_coeff == None and\\\n",
    "                                weight_constraint == None and init_mode == None:\n",
    "                model.add(Dense(int(neurons_multiplier[lay+1]),))\n",
    "                \n",
    "            elif kernel_coeff == None and bias_coeff == None and\\\n",
    "                                weight_constraint == None and init_mode != None:\n",
    "                model.add(Dense(int(neurons_multiplier[lay+1]), \n",
    "                                kernel_initializer=init_mode))\n",
    "                \n",
    "            elif kernel_coeff == None and bias_coeff == None and\\\n",
    "                                init_mode != None:\n",
    "                model.add(Dense(int(neurons_multiplier[lay+1]), \n",
    "                                kernel_initializer=init_mode,\n",
    "                                kernel_constraint=maxnorm(weight_constraint)))\n",
    "            \n",
    "            elif weight_constraint == None and init_mode != None:\n",
    "                model.add(Dense(int(neurons_multiplier[lay+1]), \n",
    "                                kernel_initializer=init_mode,\n",
    "                                kernel_regularizer=l2(kernel_coeff), \n",
    "                                bias_regularizer=l2(bias_coeff),))\n",
    "            \n",
    "            elif init_mode == None and weight_constraint == None:\n",
    "                model.add(Dense(int(neurons_multiplier[lay+1]), \n",
    "                                kernel_regularizer=l2(kernel_coeff), \n",
    "                                bias_regularizer=l2(bias_coeff),))\n",
    "                \n",
    "            else:\n",
    "                model.add(Dense(int(neurons_multiplier[lay+1]), \n",
    "                                kernel_initializer=init_mode,\n",
    "                                kernel_regularizer=l2(kernel_coeff), \n",
    "                                bias_regularizer=l2(bias_coeff), \n",
    "                                kernel_constraint=maxnorm(weight_constraint)))\n",
    "            \n",
    "            if batch_norm:\n",
    "                model.add(BatchNormalization())\n",
    "            model.add(Activation(activation))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "    ## Output layer \n",
    "    model.add(Dense(int(neurons_multiplier[-1]), activation='softmax'))\n",
    "    if optimizer == \"adam\" and learning_rate != None:\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "    else:\n",
    "        opt = optimizer\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[metricsNN])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def model_arch_general_optimized(n_bins, n_outputs, kernel_coeff = 0.0005, bias_coeff = 0.0005, lr=None, verbose=1,\n",
    "                       write_to_console=None):\n",
    "    \"\"\"\n",
    "    Very simple and straight forward Neural Network with few hyperparameters\n",
    "    straighforward RELU activation strategy with cross entropy to identify the HKL\n",
    "    Tried BatchNormalization --> no significant impact\n",
    "    Tried weighted approach --> not better for HCP\n",
    "    Trying Regularaization \n",
    "    l2(0.001) means that every coefficient in the weight matrix of the layer \n",
    "    will add 0.001 * weight_coefficient_value**2 to the total loss of the network\n",
    "    \"\"\"\n",
    "    if n_outputs >= n_bins:\n",
    "        param = n_bins\n",
    "        if param*15 < (2*n_outputs): ## quick hack; make Proper implementation\n",
    "            param = (n_bins + n_outputs)//2\n",
    "    else:\n",
    "        # param = n_outputs ## More reasonable ???\n",
    "        param = n_outputs*2 ## More reasonable ???\n",
    "        # param = n_bins//2\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(keras.Input(shape=(n_bins,)))\n",
    "    ## Hidden layer 1\n",
    "    model.add(Dense(n_bins, kernel_regularizer=l2(kernel_coeff), bias_regularizer=l2(bias_coeff)))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.3)) ## Adding dropout as we introduce some uncertain data with noise\n",
    "    ## Hidden layer 2\n",
    "    model.add(Dense(((param)*15 + n_bins)//2, kernel_regularizer=l2(kernel_coeff), bias_regularizer=l2(bias_coeff)))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    ## Hidden layer 3\n",
    "    model.add(Dense((param)*15, kernel_regularizer=l2(kernel_coeff), bias_regularizer=l2(bias_coeff)))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    ## Output layer \n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    ## Compile model\n",
    "    if lr != None:\n",
    "        otp = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=otp, metrics=[metricsNN])\n",
    "    else:\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=[metricsNN])\n",
    "    \n",
    "    if verbose == 1:\n",
    "        model.summary()\n",
    "        stringlist = []\n",
    "        model.summary(print_fn=lambda x: stringlist.append(x))\n",
    "        short_model_summary = \"\\n\".join(stringlist)\n",
    "        if write_to_console!=None:\n",
    "            write_to_console(short_model_summary)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa48403-16da-4d41-8055-3f9b696bd4bc",
   "metadata": {},
   "source": [
    "## Step 4: Training  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4ff9f01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_40 (Dense)            (None, 900)               810900    \n",
      "                                                                 \n",
      " activation_30 (Activation)  (None, 900)               0         \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (None, 900)               0         \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 884)               796484    \n",
      "                                                                 \n",
      " activation_31 (Activation)  (None, 884)               0         \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (None, 884)               0         \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 930)               823050    \n",
      "                                                                 \n",
      " activation_32 (Activation)  (None, 930)               0         \n",
      "                                                                 \n",
      " dropout_32 (Dropout)        (None, 930)               0         \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 31)                28861     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,459,295\n",
      "Trainable params: 2,459,295\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Number of spots in a batch of 50 files : 5878\n",
      "Min, Max class ID is 0, 30\n",
      "Epoch 1/5\n",
      "10/10 [==============================] - 7s 697ms/step - loss: 6.7063 - fn: 58825.0000 - fp: 115.0000 - tn: 1808135.0000 - tp: 1450.0000 - precision: 0.9265 - accuracy: 0.0241 - val_loss: 1.3187 - val_fn: 8221.0000 - val_fp: 142.0000 - val_tn: 360008.0000 - val_tp: 3784.0000 - val_precision: 0.9638 - val_accuracy: 0.3152\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 5s 523ms/step - loss: 1.9314 - fn: 26463.0000 - fp: 2374.0000 - tn: 1805876.0000 - tp: 33812.0000 - precision: 0.9344 - accuracy: 0.5610 - val_loss: 0.2788 - val_fn: 1229.0000 - val_fp: 68.0000 - val_tn: 360082.0000 - val_tp: 10776.0000 - val_precision: 0.9937 - val_accuracy: 0.8976\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 5s 566ms/step - loss: 0.6392 - fn: 7243.0000 - fp: 1375.0000 - tn: 1806875.0000 - tp: 53032.0000 - precision: 0.9747 - accuracy: 0.8798 - val_loss: 0.1554 - val_fn: 643.0000 - val_fp: 153.0000 - val_tn: 359997.0000 - val_tp: 11362.0000 - val_precision: 0.9867 - val_accuracy: 0.9464\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 5s 565ms/step - loss: 0.3580 - fn: 3742.0000 - fp: 1265.0000 - tn: 1806985.0000 - tp: 56533.0000 - precision: 0.9781 - accuracy: 0.9379 - val_loss: 0.1245 - val_fn: 507.0000 - val_fp: 176.0000 - val_tn: 359974.0000 - val_tp: 11498.0000 - val_precision: 0.9849 - val_accuracy: 0.9578\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 5s 566ms/step - loss: 0.2490 - fn: 2694.0000 - fp: 939.0000 - tn: 1807311.0000 - tp: 57581.0000 - precision: 0.9840 - accuracy: 0.9553 - val_loss: 0.0986 - val_fn: 403.0000 - val_fp: 143.0000 - val_tn: 360007.0000 - val_tp: 11602.0000 - val_precision: 0.9878 - val_accuracy: 0.9664\n",
      "Saved model to disk\n",
      "Training Accuracy: 0.9553048610687256\n",
      "Training Loss: 0.24895352125167847\n",
      "Validation Accuracy: 0.9664306640625\n",
      "Validation Loss: 0.09861008822917938\n"
     ]
    }
   ],
   "source": [
    "# load model and train\n",
    "#neurons_multiplier is a list with number of neurons per layer, the first value is input shape and last value is output shape, inbetween are the number of neurons per hidden layers\n",
    "model = model_arch_general( \n",
    "                           kernel_coeff = None, \n",
    "                           bias_coeff = None, \n",
    "                           init_mode = None,\n",
    "                           learning_rate = None, \n",
    "                           neurons_multiplier= [n_bins, n_bins, (n_outputs*2*7)+(n_bins/2), n_outputs*2*15, n_outputs], \n",
    "                           layers = 3, \n",
    "                           batch_norm = False, \n",
    "                           optimizer = \"adam\",\n",
    "                           activation = 'relu',\n",
    "                           dropout_rate = 0.3, \n",
    "                           weight_constraint = None\n",
    "                            )\n",
    "#model = model_arch_general_optimized(n_bins, n_outputs, kernel_coeff = 0.0005, bias_coeff = 0.0005, lr=None, verbose=1,)\n",
    "## temp function to quantify the spots and classes present in a batch\n",
    "batch_size = input_params[\"batch_size\"] \n",
    "trainy_inbatch = array_generator_verify(save_directory+\"//training_data\", batch_size, \n",
    "                                        len(classhkl), loc_new, print)\n",
    "print(\"Number of spots in a batch of %i files : %i\" %(batch_size, len(trainy_inbatch)))\n",
    "print(\"Min, Max class ID is %i, %i\" %(np.min(trainy_inbatch), np.max(trainy_inbatch)))\n",
    "\n",
    "epochs = input_params[\"epochs\"] \n",
    "\n",
    "## Batch loading for numpy grain files (Keep low value to avoid overcharging the RAM)\n",
    "nb_grains_per_lp1 = nb_grains_per_lp\n",
    "if material_ != material1_:\n",
    "    nb_grains_list = list(range(nb_grains_per_lp+1))\n",
    "    nb_grains1_list = list(range(nb_grains_per_lp1+1))\n",
    "    list_permute = list(itertools.product(nb_grains_list, nb_grains1_list))\n",
    "    list_permute.pop(0)\n",
    "    steps_per_epoch = (len(list_permute) * grains_nb_simulate)//batch_size\n",
    "else:\n",
    "    steps_per_epoch = int((nb_grains_per_lp * grains_nb_simulate) / batch_size)\n",
    "\n",
    "val_steps_per_epoch = int(steps_per_epoch / 5)\n",
    "if steps_per_epoch == 0:\n",
    "    steps_per_epoch = 1\n",
    "if val_steps_per_epoch == 0:\n",
    "    val_steps_per_epoch = 1 \n",
    "    \n",
    "## Load generator objects from filepaths (iterators for Training and Testing datasets)\n",
    "training_data_generator = array_generator(save_directory+\"//training_data\", batch_size, \\\n",
    "                                          len(classhkl), loc_new, print)\n",
    "testing_data_generator = array_generator(save_directory+\"//testing_data\", batch_size, \\\n",
    "                                          len(classhkl), loc_new, print)\n",
    "\n",
    "######### TRAIN THE DATA\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', patience=2)\n",
    "ms = ModelCheckpoint(save_directory+\"//best_val_acc_model.h5\", monitor='val_accuracy', \n",
    "                      mode='max', save_best_only=True)\n",
    "\n",
    "# model save directory and filename\n",
    "if material_ != material1_:\n",
    "    model_name = save_directory+\"//model_\"+material_+\"_\"+material1_\n",
    "else:\n",
    "    model_name = save_directory+\"//model_\"+material_\n",
    "\n",
    "## Fitting function\n",
    "stats_model = model.fit(\n",
    "                        training_data_generator, \n",
    "                        epochs=epochs, \n",
    "                        steps_per_epoch=steps_per_epoch,\n",
    "                        validation_data=testing_data_generator,\n",
    "                        validation_steps=val_steps_per_epoch,\n",
    "                        verbose=1,\n",
    "                        class_weight=class_weights,\n",
    "                        callbacks=[es, ms]\n",
    "                        )\n",
    "\n",
    "# Save model config and weights\n",
    "model_json = model.to_json()\n",
    "with open(model_name+\".json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)            \n",
    "# serialize weights to HDF5\n",
    "model.save_weights(model_name+\".h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "print( \"Training Accuracy: \"+str( stats_model.history['accuracy'][-1]))\n",
    "print( \"Training Loss: \"+str( stats_model.history['loss'][-1]))\n",
    "print( \"Validation Accuracy: \"+str( stats_model.history['val_accuracy'][-1]))\n",
    "print( \"Validation Loss: \"+str( stats_model.history['val_loss'][-1]))\n",
    "\n",
    "# Plot the accuracy/loss v Epochs\n",
    "epochs = range(1, len(model.history.history['loss']) + 1)\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].plot(epochs, model.history.history['loss'], 'r', label='Training loss')\n",
    "ax[0].plot(epochs, model.history.history['val_loss'], 'r', ls=\"dashed\", label='Validation loss')\n",
    "ax[0].legend()\n",
    "ax[1].plot(epochs, model.history.history['accuracy'], 'g', label='Training Accuracy')\n",
    "ax[1].plot(epochs, model.history.history['val_accuracy'], 'g', ls=\"dashed\", label='Validation Accuracy')\n",
    "ax[1].legend()\n",
    "if material_ != material1_:\n",
    "    plt.savefig(save_directory+\"//loss_accuracy_\"+material_+\"_\"+material1_+\".png\", bbox_inches='tight',format='png', dpi=1000)\n",
    "else:\n",
    "    plt.savefig(save_directory+\"//loss_accuracy_\"+material_+\".png\", bbox_inches='tight',format='png', dpi=1000)\n",
    "plt.close()\n",
    "\n",
    "if material_ != material1_:\n",
    "    text_file = open(save_directory+\"//loss_accuracy_logger_\"+material_+\"_\"+material1_+\".txt\", \"w\")\n",
    "else:\n",
    "    text_file = open(save_directory+\"//loss_accuracy_logger_\"+material_+\".txt\", \"w\")\n",
    "\n",
    "text_file.write(\"# EPOCH, LOSS, VAL_LOSS, ACCURACY, VAL_ACCURACY\" + \"\\n\")\n",
    "for inj in range(len(epochs)):\n",
    "    string1 = str(epochs[inj]) + \",\"+ str(model.history.history['loss'][inj])+\\\n",
    "            \",\"+str(model.history.history['val_loss'][inj])+\",\"+str(model.history.history['accuracy'][inj])+\\\n",
    "            \",\"+str(model.history.history['val_accuracy'][inj])+\" \\n\"  \n",
    "    text_file.write(string1)\n",
    "text_file.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8f1b59-830b-42d3-9c20-74663c27b136",
   "metadata": {},
   "source": [
    "## Stats on the trained model with sklearn metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e7bc9a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        84\n",
      "           1       0.97      1.00      0.99       179\n",
      "           2       0.97      0.99      0.98       114\n",
      "           3       0.99      0.97      0.98       320\n",
      "           4       0.98      0.97      0.98       317\n",
      "           5       0.99      0.98      0.98       266\n",
      "           6       0.98      0.98      0.98       255\n",
      "           7       0.99      0.97      0.98       359\n",
      "           8       0.98      0.98      0.98       220\n",
      "           9       0.98      0.99      0.99       390\n",
      "          10       0.98      0.96      0.97       167\n",
      "          11       0.99      0.96      0.98       339\n",
      "          12       0.97      0.99      0.98       108\n",
      "          13       0.98      0.98      0.98       168\n",
      "          14       0.97      0.97      0.97       167\n",
      "          15       0.99      0.98      0.98       255\n",
      "          16       1.00      0.99      0.99        74\n",
      "          17       0.96      0.98      0.97       140\n",
      "          18       0.99      0.98      0.98        91\n",
      "          19       0.94      1.00      0.97        15\n",
      "          20       0.89      1.00      0.94        17\n",
      "          21       0.94      0.99      0.97        84\n",
      "          22       0.99      0.97      0.98       296\n",
      "          23       0.98      1.00      0.99        59\n",
      "          24       0.94      0.99      0.97        98\n",
      "          25       0.96      0.96      0.96        25\n",
      "          26       0.95      1.00      0.98        20\n",
      "          27       0.98      0.97      0.98       549\n",
      "          28       0.97      0.98      0.98       243\n",
      "          29       0.94      1.00      0.97       203\n",
      "          30       0.99      0.99      0.99       197\n",
      "\n",
      "    accuracy                           0.98      5819\n",
      "   macro avg       0.97      0.98      0.98      5819\n",
      "weighted avg       0.98      0.98      0.98      5819\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "## verify the \n",
    "x_test, y_test = vali_array(save_directory+\"//testing_data\", 50, len(classhkl), loc_new, print)\n",
    "y_test = np.argmax(y_test, axis=-1)\n",
    "y_pred = np.argmax(model.predict(x_test), axis=-1)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0bb648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
