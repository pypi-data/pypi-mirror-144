{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61bda473-6e51-4914-826e-21c50f0f61d1",
   "metadata": {},
   "source": [
    "# Notebook script for Grid search optimization of hyperperameters of neural network architecture\n",
    "\n",
    "### Load the data generated in Step 1\n",
    "### Define the Neural network architecture (with hyper parameters to be optimized)\n",
    "### Run Sklearn GridSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2ad7bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import modules used for this Notebook\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## if LaueToolsNN is properly installed\n",
    "try:\n",
    "    from lauetoolsnn.utils_lauenn import vali_array\n",
    "except:\n",
    "    # else import from a path where LaueToolsNN files are\n",
    "    import sys\n",
    "    sys.path.append(r\"C:\\Users\\purushot\\Desktop\\github_version_simple\\lauetoolsnn\")\n",
    "    from utils_lauenn import vali_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa0a493-5a72-4c14-be8d-01d54f073260",
   "metadata": {},
   "source": [
    "## step 1: define material and path to access the training dataset generated using Step 1 script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebdbbba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory where training dataset is stored is : C:\\Users\\purushot\\Desktop\\github_version_simple\\lauetoolsnn\\example_notebook_scripts//Cu\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "## User Input dictionary with parameters (reduced but same as the one used in STEP 1)\n",
    "## In case of only one phase/material, keep same value for material_ and material1_ key\n",
    "# =============================================================================\n",
    "input_params = {\n",
    "                \"material_\": \"Cu\",             ## same key as used in dict_LaueTools\n",
    "                \"material1_\": \"Cu\",            ## same key as used in dict_LaueTools\n",
    "                \"prefix\" : \"\",                 ## prefix for the folder to be created for training dataset\n",
    "                \"nb_grains_per_lp\" : 5,        ## max grains to be generated in a Laue Image\n",
    "                \"grains_nb_simulate\" : 100,    ## Number of orientations to generate (takes advantage of crystal symmetry)\n",
    "                \"batch_size\":50,               ## batches of files to use while training\n",
    "                \"epochs\":5,                    ## number of epochs for training\n",
    "                }\n",
    "\n",
    "material_= input_params[\"material_\"]\n",
    "material1_= input_params[\"material1_\"]\n",
    "nb_grains_per_lp = input_params[\"nb_grains_per_lp\"]\n",
    "grains_nb_simulate = input_params[\"grains_nb_simulate\"]\n",
    "\n",
    "if material_ != material1_:\n",
    "    save_directory = os.getcwd()+\"//\"+material_+\"_\"+material1_+input_params[\"prefix\"]\n",
    "else:\n",
    "    save_directory = os.getcwd()+\"//\"+material_+input_params[\"prefix\"]\n",
    "\n",
    "if not os.path.exists(save_directory):\n",
    "    print(\"The directory doesn't exists; please veify the path\")\n",
    "else:\n",
    "    print(\"Directory where training dataset is stored is : \"+save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be7b1f5-2ac1-45f6-8245-439ced9081d4",
   "metadata": {},
   "source": [
    "## Step 2: Load the necessary files generated in Step 1 script\n",
    "### Loading the Output class and ground truth; loading the training dataset of user defined batch number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4155ee36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900 31\n",
      "Number of spots in a batch of 20 files : 2591\n",
      "Min, Max class ID is 0, 30\n"
     ]
    }
   ],
   "source": [
    "classhkl = np.load(save_directory+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_0\"]\n",
    "angbins = np.load(save_directory+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_1\"]\n",
    "loc_new = np.load(save_directory+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_2\"]\n",
    "n_bins = len(angbins)-1\n",
    "n_outputs = len(classhkl)\n",
    "print(n_bins, n_outputs)\n",
    "      \n",
    " ## Tuning on small dataset of Cu (20 corresponds to number of files)\n",
    "x_training, y_training = vali_array(save_directory+\"//training_data\", 20, \n",
    "                                    len(classhkl), \n",
    "                                    loc_new, print, tocategorical=False)\n",
    "print(\"Number of spots in a batch of %i files : %i\" %(20, len(x_training)))\n",
    "print(\"Min, Max class ID is %i, %i\" %(np.min(y_training), np.max(y_training)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8be2bdc-587e-47d0-b4c1-9d32fe78afc2",
   "metadata": {},
   "source": [
    "## Step 3: Defining a neural network architecture with hyperparameters as free parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8e577ef-7f75-4ef0-b873-57607e34e6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import keras\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "####################################################################################\n",
    "## General architecture with all free parameters\n",
    "####################################################################################\n",
    "def model_arch_general(kernel_coeff = 0.0005, \n",
    "                       bias_coeff = 0.0005, \n",
    "                       init_mode='uniform',\n",
    "                       learning_rate=0.0001, \n",
    "                       neurons_multiplier= [1200, 1200, (32*2*7)+(1200//2), (32*2*15), 32], \n",
    "                       layers=3, \n",
    "                       batch_norm=False, \n",
    "                       optimizer=\"adam\",\n",
    "                       activation='relu',\n",
    "                       dropout_rate=0.0, \n",
    "                       weight_constraint=0):\n",
    "    model = Sequential()\n",
    "    # Input layer\n",
    "    model.add(keras.Input(shape=(int(neurons_multiplier[0]),)))\n",
    "    \n",
    "    if layers > 0:\n",
    "        for lay in range(layers):\n",
    "            ## Hidden layer n\n",
    "            if kernel_coeff == None and bias_coeff == None and\\\n",
    "                                weight_constraint == None and init_mode == None:\n",
    "                model.add(Dense(int(neurons_multiplier[lay+1]),))\n",
    "                \n",
    "            elif kernel_coeff == None and bias_coeff == None and\\\n",
    "                                weight_constraint == None and init_mode != None:\n",
    "                model.add(Dense(int(neurons_multiplier[lay+1]), \n",
    "                                kernel_initializer=init_mode))\n",
    "                \n",
    "            elif kernel_coeff == None and bias_coeff == None and\\\n",
    "                                init_mode != None:\n",
    "                model.add(Dense(int(neurons_multiplier[lay+1]), \n",
    "                                kernel_initializer=init_mode,\n",
    "                                kernel_constraint=maxnorm(weight_constraint)))\n",
    "            \n",
    "            elif weight_constraint == None and init_mode != None:\n",
    "                model.add(Dense(int(neurons_multiplier[lay+1]), \n",
    "                                kernel_initializer=init_mode,\n",
    "                                kernel_regularizer=l2(kernel_coeff), \n",
    "                                bias_regularizer=l2(bias_coeff),))\n",
    "            \n",
    "            elif init_mode == None and weight_constraint == None:\n",
    "                model.add(Dense(int(neurons_multiplier[lay+1]), \n",
    "                                kernel_regularizer=l2(kernel_coeff), \n",
    "                                bias_regularizer=l2(bias_coeff),))\n",
    "                \n",
    "            else:\n",
    "                model.add(Dense(int(neurons_multiplier[lay+1]), \n",
    "                                kernel_initializer=init_mode,\n",
    "                                kernel_regularizer=l2(kernel_coeff), \n",
    "                                bias_regularizer=l2(bias_coeff), \n",
    "                                kernel_constraint=maxnorm(weight_constraint)))\n",
    "            \n",
    "            if batch_norm:\n",
    "                model.add(BatchNormalization())\n",
    "            model.add(Activation(activation))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "    ## Output layer \n",
    "    model.add(Dense(int(neurons_multiplier[-1]), activation='softmax'))\n",
    "    if optimizer == \"adam\" and learning_rate != None:\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "    else:\n",
    "        opt = optimizer\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa48403-16da-4d41-8055-3f9b696bd4bc",
   "metadata": {},
   "source": [
    "## Step 4: Define hyperparameters and launch grid search  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ff9f01f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13524\\3466116334.py\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[0mgrid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_ann\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;31m# Fit grid search\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[0mgrid_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_training\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_training\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[1;31m# View hyperparameters of best neural network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m# summarize results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\laueNN\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    889\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\laueNN\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1390\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1391\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1392\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\laueNN\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    836\u001b[0m                     )\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 838\u001b[1;33m                 out = parallel(\n\u001b[0m\u001b[0;32m    839\u001b[0m                     delayed(_fit_and_score)(\n\u001b[0;32m    840\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\laueNN\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1056\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1057\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\laueNN\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    933\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 935\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    936\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\laueNN\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\laueNN\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    439\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\laueNN\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "if __name__ == '__main__':  # Multiprocessing enclosing\n",
    "    if material_ != material1_:\n",
    "        text_file = open(save_directory+\"//grid_optimizer_logger_\"+material_+\"_\"+material1_+\".txt\", \"w\")\n",
    "    else:\n",
    "        text_file = open(save_directory+\"//grid_optimizer_logger_\"+material_+\".txt\", \"w\")\n",
    "\n",
    "    # =============================================================================\n",
    "    # Tuning neural network parameters\n",
    "    # Play with fixing and freeing different parameters\n",
    "    # =============================================================================\n",
    "\n",
    "    # Wrap Keras model so it can be used by scikit-learn\n",
    "    model_ann = KerasClassifier(build_fn=model_arch_general, verbose=2)\n",
    "    # Create hyperparameter space\n",
    "    optimizer = ['Adam']\n",
    "    learning_rate = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "    init_mode = [None] #['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "    activation = ['relu'] # ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "    dropout_rate = [0.0, 0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    weight_constraint = [None] #[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    layers = [1,2,3,4]\n",
    "    values = [None]\n",
    "\n",
    "    first_layer = int(n_outputs * 2)\n",
    "    neurons_multiplier = [[n_bins, n_bins, first_layer*5, first_layer*10, first_layer*15, n_outputs],\n",
    "                          [n_bins, n_bins, first_layer*10, first_layer*15, first_layer*20, n_outputs],\n",
    "                          [n_bins, n_bins, first_layer*15, first_layer*20, first_layer*25, n_outputs],\n",
    "                          [n_bins, n_bins, first_layer//4, first_layer//5, first_layer//6, n_outputs],\n",
    "                          [n_bins, n_bins, first_layer, first_layer, first_layer, n_outputs],\n",
    "                          [n_bins, n_bins, 32, 64, 128, n_outputs],\n",
    "                          [n_bins, n_bins, 32, 64, 32, n_outputs],\n",
    "                          [n_bins, n_bins, n_bins, n_bins, n_bins, n_outputs],]\n",
    "    batch_norm = [False]\n",
    "\n",
    "    # Create hyperparameter options\n",
    "    hyperparameters = dict( \n",
    "                            kernel_coeff=values,\n",
    "                            bias_coeff=values,\n",
    "                            init_mode=init_mode,\n",
    "                            learning_rate=learning_rate,\n",
    "                            neurons_multiplier=neurons_multiplier, \n",
    "                            layers=layers,\n",
    "                            batch_norm=batch_norm,\n",
    "                            optimizer=optimizer,\n",
    "                            activation=activation,\n",
    "                            dropout_rate=dropout_rate,\n",
    "                            weight_constraint=weight_constraint,\n",
    "                            )\n",
    "\n",
    "    grid = GridSearchCV(estimator=model_ann, cv=5, param_grid=hyperparameters, n_jobs=-1)\n",
    "    # Fit grid search\n",
    "    grid_result = grid.fit(x_training, y_training)\n",
    "    # View hyperparameters of best neural network\n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    text_file.write(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_) + \"\\n\")\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "        text_file.write(\"%f (%f) with: %r\" % (mean, stdev, param) + \"\\n\")\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0bb648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
