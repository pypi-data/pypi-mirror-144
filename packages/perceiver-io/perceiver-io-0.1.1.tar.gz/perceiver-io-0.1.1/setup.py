# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['perceiver', 'perceiver.data', 'perceiver.model', 'perceiver.scripts']

package_data = \
{'': ['*']}

install_requires = \
['einops>=0.4.0,<0.5.0',
 'fairscale>=0.4.0,<0.5.0',
 'jsonargparse[signatures]>=4.1.0,<4.2.0',
 'lightning-bolts>=0.4.0,<0.5.0',
 'pytorch-lightning>=1.5.0,<1.6.0',
 'tokenizers>=0.11.0,<0.12.0',
 'torch>=1.10.0,<1.11.0',
 'torchmetrics>=0.6.0,<0.7.0',
 'torchtext>=0.11.0,<0.12.0',
 'torchvision>=0.11.0,<0.12.0']

setup_kwargs = {
    'name': 'perceiver-io',
    'version': '0.1.1',
    'description': 'Perceiver IO',
    'long_description': "# Perceiver IO\n\nA PyTorch implementation of\n\n- [Perceiver: General Perception with Iterative Attention](https://arxiv.org/abs/2103.03206)\n- [Perceiver IO: A General Architecture for Structured Inputs & Outputs](https://arxiv.org/abs/2107.14795)\n\nThis project supports training of Perceiver IO models with [Pytorch Lightning](https://www.pytorchlightning.ai/).\nTraining examples are given in section [Tasks](#tasks), inference examples in section [Notebooks](#notebooks).\nPerceiver IO models are constructed with generic encoder and decoder classes and task-specific input and\noutput adapters (see [Model API](#model-api)). The command line interface is implemented with\n[Lighting CLI](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_cli.html).\n\n\n## Setup\n\n```shell\nconda env create -f environment.yml\nconda activate perceiver-io\npoetry install\n```\n\n## Tasks\n\nIn the following subsections, Perceiver IO models are trained on a rather small scale. In particular, hyperparameters\nare set such that parallel training on two NVIDIA GTX 1080 GPUs (8 GB memory each) works quite well. I didn't really\ntune model architectures and other hyperparameters, so you'll probably get better results with a bit of experimentation.\nSupport for more datasets and tasks will be added later.\n\n### Masked language modeling\n\nPretrain a Perceiver IO model on masked language modeling (MLM) with text from the IMDB training set. The pretrained\nencoder is then used for training a [sentiment classification](#sentiment-classification) model.\n[Predictions of masked tokens](docs/tensorboard.md) are logged to Tensorboard.\n\n```shell\npython perceiver/scripts/mlm.py fit \\\n  --model.num_latent_channels=64 \\\n  --model.encoder.num_layers=3 \\\n  --model.encoder.dropout=0.0 \\\n  --model.decoder.dropout=0.0 \\\n  --data=IMDBDataModule \\\n  --data.max_seq_len=512 \\\n  --data.batch_size=64 \\\n  --optimizer.lr=3e-3 \\\n  --optimizer.weight_decay=0.0 \\\n  --lr_scheduler.pct_start=0.1 \\\n  --trainer.accelerator=gpu \\\n  --trainer.devices=-1 \\\n  --trainer.max_steps=50000 \\\n  --trainer.check_val_every_n_epoch=5\n```\n\nFor saving GPU memory and scaling model training, [activation checkpointing](docs/checkpointing.md) can be enabled with\n`--model.activation_checkpoint=true` (disabled by default).\n\n### Sentiment classification\n\nTrain a classification decoder using a frozen encoder from [masked language modeling](#masked-language-modeling-mlm).\nIf you ran MLM yourself you'll need to modify the `--model.mlm_ckpt` argument accordingly, otherwise download\ncheckpoints from [here](https://martin-krasser.com/perceiver/logs-update-2.zip) and extract them in the root directory of\nthis project.\n\n```shell\npython perceiver/scripts/seq_clf.py fit \\\n  --model.mlm_ckpt='logs/mlm/version_0/checkpoints/epoch=254-val_loss=4.556.ckpt' \\\n  --model.num_latent_channels=64 \\\n  --model.encoder.num_layers=3 \\\n  --model.encoder.freeze=true \\\n  --model.encoder.dropout=0.0 \\\n  --model.decoder.dropout=0.0 \\\n  --data=IMDBDataModule \\\n  --data.max_seq_len=512 \\\n  --data.batch_size=128 \\\n  --optimizer.lr=1e-3 \\\n  --optimizer.weight_decay=0.01 \\\n  --trainer.accelerator=gpu \\\n  --trainer.devices=-1 \\\n  --trainer.max_epochs=30\n```\n\nUnfreeze the encoder and jointly fine-tune it together with the decoder that has been trained in the previous step.\nIf you ran the previous step yourself you'll need to modify the `--model.clf_ckpt` argument accordingly, otherwise\ndownload checkpoints from [here](https://martin-krasser.com/perceiver/logs-update-2.zip).\n\n```shell\npython perceiver/scripts/seq_clf.py fit \\\n  --model.clf_ckpt='logs/seq_clf/version_0/checkpoints/epoch=024-val_loss=0.352.ckpt' \\\n  --model.num_latent_channels=64 \\\n  --model.encoder.num_layers=3 \\\n  --model.encoder.dropout=0.1 \\\n  --model.decoder.dropout=0.1 \\\n  --data=IMDBDataModule \\\n  --data.max_seq_len=512 \\\n  --data.batch_size=128 \\\n  --optimizer.lr=1e-4 \\\n  --optimizer.weight_decay=0.01 \\\n  --trainer.accelerator=gpu \\\n  --trainer.devices=-1 \\\n  --trainer.max_epochs=30\n```\n\n### Image classification\n\nClassify MNIST images. See also [Model API](#model-api) for details about the underlying Perceiver IO model.\n\n```shell\npython perceiver/scripts/img_clf.py fit \\\n  --model.num_latent_channels=128 \\\n  --model.encoder.num_layers=3 \\\n  --model.encoder.dropout=0.0 \\\n  --model.decoder.dropout=0.0 \\\n  --data=MNISTDataModule \\\n  --data.batch_size=128 \\\n  --optimizer.lr=1e-3 \\\n  --optimizer.weight_decay=0.01 \\\n  --trainer.accelerator=gpu \\\n  --trainer.devices=-1 \\\n  --trainer.max_epochs=20\n```\n\n## Notebooks\n\n- [Image classification](notebooks/img-clf.ipynb)\n- [Sentiment classification](notebooks/txt-clf.ipynb)\n\nStart the notebook server with:\n\n```shell\nPYTHONPATH=.. jupyter notebook\n```\n\n## Model API\n\nThe [model](perceiver/model/model.py) API is based on generic encoder and decoder classes (`PerceiverEncoder` and\n`PerceiverDecoder`) and task-specific input and output [adapters](perceiver/model/adapter.py). The following snippet\nshows how they can be used to create an MNIST image classifier, for example:\n\n```python\nfrom perceiver.model import (\n    PerceiverIO,\n    PerceiverEncoder,\n    PerceiverDecoder,\n    ImageInputAdapter,\n    ClassificationOutputAdapter,\n)\n\n# Fourier-encode pixel positions and flatten along spatial dimensions\ninput_adapter = ImageInputAdapter(image_shape=(28, 28, 1), num_frequency_bands=32)\n\n# Project generic Perceiver decoder output to specified number of classes\noutput_adapter = ClassificationOutputAdapter(num_classes=10, num_output_channels=128)\n\n# Generic Perceiver encoder\nencoder = PerceiverEncoder(\n    input_adapter=input_adapter,\n    num_latents=32,\n    num_latent_channels=128,\n    num_layers=3,\n    num_cross_attention_heads=4,\n    num_self_attention_heads=4,\n    num_self_attention_layers_per_block=3,\n    dropout=0.0,\n)\n\n# Generic Perceiver decoder\ndecoder = PerceiverDecoder(\n    output_adapter=output_adapter,\n    num_latent_channels=128,\n    num_cross_attention_heads=1,\n    dropout=0.0,\n)\n\n# MNIST classifier implemented as Perceiver IO model\nmnist_classifier = PerceiverIO(encoder, decoder)\n```\n\n## Development environment\n\nUpdate the project dependencies in the conda environment:\n\n```bash\ninvoke install\n```\n\nInstall the pre-commit hooks:\n\n```bash\ninvoke precommit-install\n```\n\nRun code quality checks:\n\n```bash\ninvoke cc\n```\n\nRun tests:\n\n```bash\ninvoke test\n```\n\nThe project and task structure presented here is based on the [Python Project Template](https://github.com/cstub/python-project-template).\n\n## Citations\n\n```bibtex\n@misc{jaegle2021perceiver,\n    title   = {Perceiver: General Perception with Iterative Attention},\n    author  = {Andrew Jaegle and Felix Gimeno and Andrew Brock and Andrew Zisserman and Oriol Vinyals and Joao Carreira},\n    year    = {2021},\n    eprint  = {2103.03206},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{jaegle2021perceiver,\n    title   = {Perceiver IO: A General Architecture for Structured Inputs & Outputs},\n    author  = {Andrew Jaegle and Sebastian Borgeaud and Jean-Baptiste Alayrac and Carl Doersch and Catalin Ionescu and David Ding and Skanda Koppula and Andrew Brock and Evan Shelhamer and Olivier Hénaff and Matthew M. Botvinick and Andrew Zisserman and Oriol Vinyals and João Carreira},\n    year    = {2021},\n    eprint  = {2107.14795},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}\n```\n",
    'author': 'Martin Krasser',
    'author_email': 'krasserm@googlemail.com',
    'maintainer': None,
    'maintainer_email': None,
    'url': 'https://github.com/krasserm/perceiver-io',
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'python_requires': '>=3.9,<4.0',
}


setup(**setup_kwargs)
