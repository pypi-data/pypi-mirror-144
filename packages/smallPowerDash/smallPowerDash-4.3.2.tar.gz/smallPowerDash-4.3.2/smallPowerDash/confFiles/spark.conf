#!/bin/bash

################
# Spark config #
################
set -o allexport

# SPARK HOME
SPARK_HOME="/opt/spark/spark-3.0.1-bin-hadoop2.7"

# SPARK PYTHON VARIABLES
PYTHONPATH="${SPARK_HOME}/python/:${SPARK_HOME}/python/lib/py4j-0.10.9-src.zip"
PYTHONSTARTUP="${SPARK_HOME}/python/pyspark/shell.py"
PYSPARK_PYTHON="${ANACONDA_HOME}/bin/python"

# SPARK CONFIG FOR LOCAL
SPARK_MASTER="local[*]"
SPARK_DEPLOY_MODE="client"
SPARK_CONFIG="\
--conf spark.port.maxRetries=1000 \
--conf spark.sql.hive.thriftServer.singleSession=true \
--conf spark.eventLog.enabled=false \
--conf spark.executor.extraJavaOptions=-Djava.io.tmpdir=${APPLICATION_HOME}/tmp \
--conf spark.driver.extraJavaOptions=-Djava.io.tmpdir=${APPLICATION_HOME}/tmp \
--conf spark.driver.memory=6g \
--conf spark.driver.maxResultSize=5g \
--conf spark.executor.memory=5g \
--conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
--conf spark.sql.autoBroadcastJoinThreshold=10485760 \
--conf spark.sql.shuffle.partitions=800 \
--conf spark.sql.inMemoryColumnarStorage.compressed=true \
--conf spark.sql.inMemoryColumnarStorage.batchSize=10000 \
--conf spark.sql.tungsten.enabled=true \
--conf spark.sql.analyzer.maxIterations=500 \
--conf spark.memory.offHeap.enabled=true \
--conf spark.memory.offHeap.size=1g \
--conf spark.io.compression.codec=snappy \
--conf spark.dynamicAllocation.enabled=true \
--conf spark.dynamicAllocation.initialExecutors=6 \
--conf spark.dynamicAllocation.maxExecutors=6 \
--conf spark.dynamicAllocation.minExecutors=6 \
--conf spark.memory.fraction=0.75 \
--conf spark.memory.storageFraction=0.75\
"
# Examples on how to load new packages
#--packages org.mongodb.spark:mongo-spark-connector_2.11:2.2.7 \
#--packages org.apache.spark:spark-avro_2.11:2.4.1,\

# LIST OF JDBC drivers
if [ -d "${APPLICATION_HOME}/lib/jdbc" ]; then
        JDBCS=`find ${APPLICATION_HOME}/lib/jdbc -name "*.jar"`
        JDBCS=`echo ${JDBCS} | tr ' ' ','`
        if [ ! "${JDBCS}" = "" ]; then
            SPARK_CONFIG="${SPARK_CONFIG} --driver-class-path ${JDBCS}"
        fi
fi

# LIST OF JARS
if [ -d "${APPLICATION_HOME}/lib/jar" ]; then
        JARS=`find ${APPLICATION_HOME}/lib/jar -name "*.jar"`
        export JARS=`echo ${JARS} | tr ' ' ','`
        if [ ! "${JARS}" = "" ]; then
                if [ ! "${JDBCS}" = "" ]; then
                        export JARS="${JARS},${JDBCS}"
                fi
                SPARK_CONFIG="${SPARK_CONFIG} --jars ${JARS}"
        fi
fi

# SPARK COMMAND
SPARK_CMD="${SPARK_HOME}/bin/spark-submit \
--master \"${SPARK_MASTER}\" \
--deploy-mode \"${SPARK_DEPLOY_MODE}\" \
--name \"${JOB_NAME}\" \
${SPARK_CONFIG}"

set +o allexport
env
